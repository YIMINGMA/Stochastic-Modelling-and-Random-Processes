\section{Stochastic Processes}

\begin{definition}[Stochastic Processes]
    A \textbf{stochastic process} is a collection of random variables. That is, for each $t \in T$, $X(t)$ is a random variable.
\end{definition}

\begin{remark}
    The set $T$ is called the \textbf{index set} of the process.
    \begin{itemize}
        \item When $T$ is a countable set, the stochastic process is said to be a \textbf{discrete-time} process. 
        \item If $T$ is an uncountable set, such as an interval of the real line, the stochastic process is said to be a \textbf{continuous-time} process.
    \end{itemize}
\end{remark}

\begin{remark}
    The index $t$ is often interpreted as time, and as a result, we refer $X(t)$ as the \textbf{state} of the process at time $t$. And, the \textbf{state space} of a stochastic process is defined as the set of all possible values that the random variables $X(t)$ can assume.
\end{remark}




\section{Discrete-Time Markov Chains}

Consider a process that has a value in each time period. Let $X_n$ denote its value in time period $n$, and suppose we want to make a probability model for the sequence of successive values $X_0, X_1, X_2, \cdots$. The simplest model would probably be to assume that the $X_n$ are independent random variables, but often such an assumption is clearly unjustified. For instance, starting at some time suppose that $X_n$ represents the price of one share of some security, such as Google, at the end ot $n$ additional trading days. Then it certainly seems unreasonable to suppose that the price at the end of day $n+1$ is independent of the prices on days $n$, $n-1$, $n-2$ and so down to day $0$. However, it might be reasonable to suppose that the price at the end of trading day $n+1$ depends on the previous end-of-day prices only throught the price at the end of day $n$. That is, it might be reasonable to assume that the conditional distribution of $X_{n+1}$ given all the past end-of-day prices $X_n, X_{n-1}, \cdots, X_0$ depends on these past prices only through the price at the end of day $n$. Such an assumption defines a Markov chain, a type of stochastic process that will be studied in this chapter, and which we now formally define.

\begin{definition}[Markov Chains]
    Let $\{X_n, n = 0, 1, 2, \cdots\}$ be a stochastic process that takes on a finite or countable number of possible values. Denote the set of all these values by $S$. The process is called a \textbf{Markov chain} if for all $A \subset S$, $n \in \mbb{N}$ and $s_0, \cdots, s_n \in S$,
    $$\mbb{P}[X_{n+1} \in A | X_n = s_n, \cdots, X_0 = s_0] = \mbb{P}[X_{n+1} \in A | X_n = s_n].$$
\end{definition}

\begin{itemize}
    \item $S$ is called the \textbf{state space} of the process.
    \item If $X_n = i$, for some $n \in \mbb{N}$ and $i \in S$, then the process is said to be in state $i$ at time $n$.
    \item The property that the the conditional probability distribution of future states of the process (conditional on both past and present states) depends only upon the present state, not on the sequence of events that preceded it, is called the \textbf{Markov property}.
\end{itemize}

\begin{definition}[Homogeneity]
    A Markov process is called \textbf{homogeneous} if 
    $$\mbb{P}[X_{n+1} = a | X_n = b],$$
    $\forall a, b \in S$, is independent of $n$.
\end{definition}

\begin{theorem}
    Markov property implies the past variable $X_{n-1}$ and the future variable $X_{n+1}$ are independent conditional on the present $X_n$.
    \begin{proof}
        \begin{align*}
            & \mbb{P}[X_{n-1} = s_{n-1}, X_{n+1} = s_{n+1} | X_n = s_n] \\ 
            = & \frac{\mbb{P}[X_{n-1} = s_{n-1}, X_n = s_n, X_{n+1} = s_{n+1}]}{\mbb{P}[ X_n = s_n]} \\ 
            = & \frac{\mbb{P}[X_{n-1} = s_{n-1}]}{\mbb{P}[ X_n = s_n]}  \mbb{P}[X_n = s_n| X_{n-1} = s_{n-1}]  \mbb{P} [X_{n+1} = s_{n+1} | X_n = s_n, X_{n-1} = s_{n-1}] \\ 
            = & \frac{\mbb{P}[X_{n-1} = s_{n-1}]}{\mbb{P}[ X_n = s_n]}  \mbb{P}[X_n = s_n| X_{n-1} = s_{n-1}]  \mbb{P} [X_{n+1} = s_{n+1} | X_n = s_n] \\ 
            = &  \frac{\mbb{P}[X_{n-1} = s_{n-1}]}{\mbb{P}[ X_n = s_n]}  \frac{\mbb{P}[X_{n-1} = s_{n-1} | X_n = s_n]  \mbb{P}[X_n = s_n]}{\mbb{P}[X_{n-1} = s_{n-1}]}  \mbb{P} [X_{n+1} = s_{n+1} | X_n = s_n] \\ 
            = & \mbb{P}[X_{n-1} = s_{n-1} | X_n = s_n]  \mbb{P} [X_{n+1} = s_{n+1} | X_n = s_n].
        \end{align*}
    \end{proof}
\end{theorem}

\begin{definition}[Transition Matrices]
    If a discrete-time Markov chain is homogeneous, then we can define its \textbf{transition matrix} $P$ by setting
    $$P_{i,j} = \mbb{P}[X_{n+1} = j | X_n = i].$$
\end{definition}

\begin{itemize}
    \item The transition matrix of a homogeneous discrete-time Markov chain is made of constants due to homogeneity.
    \item The dimension of the transition matrix is $\#S \times \#S$, where $\#S$ denotes the size of the state space.
    \item The sum of each row of $P$ is $1$: $$\sum_{j \in S} P_{i,j} = 1.$$ So $\vec{1}$ is an eigenvector of $P$ with eigenvalue $1$.
\end{itemize}

\begin{example}[Forecasting the Weather]\label{example 4.1}
    Suppose that the chance of rain tomorrow
    depends on previous weather conditions only through whether or not it is raining today
    and not on past weather conditions. Suppose also that if it rains today, then it will rain
    tomorrow with probability $\alpha$; and if it does not rain today, then it will rain tomorrow
    with probability $\beta$.
    If we say that the process is in state $0$ when it rains and state $1$ when it does not
    rain, then the preceding is a two-state Markov chain whose transition probabilities are
    given by
    $$
    \begin{bmatrix}
        \alpha & 1-\alpha \\ 
        \beta & 1-\beta
    \end{bmatrix}.
    $$
\end{example}

\begin{example}[Random Walk with Boundaries]
    Let $\{X_n: n \in \mbb{N}\}$ be a simple random walk on $S = \{1, \cdots, L\}$ with

    $$
    \mbb{P}[X_{n+1} = j | X_n = i] = 
    \begin{cases}
        p & \qquad j = i+1,\, 1 \le i \le L-1 \\ 
        q & \qquad j = i-1,\, 2 \le i \le L \\ 
        0 & \qquad \text{Otherwise}
    \end{cases}.
    $$

    The boundary conditions are 
    \begin{itemize}
        \item \textbf{periodic} if $P_{L,1} = p,\, P_{1, L} = q$;
        \item \textbf{absorbing} if $P_{L,L} = 1,\, P_{1,1} = 1$;
        \item \textbf{closed} if $P_{L,L} = p,\, P_{1,1} = q$;
        \item \textbf{reflecting} if $P_{L, L-1} = 1$, $P_{1, 2} = 1$.
    \end{itemize}
\end{example}

\begin{definition}[Absorbing Stetes]
    A state $s \in S$ is called \textbf{absorbing} for a discrete-time Markov chain with transition matrix $P$, if 
    $$P_{s, y} = \delta_{s, y}, \, \text{ for all $y \in S$}.$$
\end{definition}

\begin{example}[Random Walk with Absorbing Boundary Conditions]
    Consider the simple random walk model $\{X_n: n \in \mbb{N}\}$ on $S = \{1, \cdots L\}$, with probability $p$ of moving right and probability $q$ of moving left, and also assume the boundary conditions are absorbing. Let $h_k$ be the absorption probability for $X_0 = k \in S$, i.e. 
    $$
    h_k = \mbb{P} [X_n \in \{1, L\} \text{ for some } n \ge 0 | X_0 = k].
    $$
    Conditioning on the first jump, we have 
    \begin{align*}
        h_k = & p \times \mbb{P} [X_n \in \{1, L\} \text{ for some } n \ge 0 | X_0 = k, X_1 = k+1] \\ 
        & + q  \times\mbb{P} [X_n \in \{1, L\} \text{ for some } n \ge 0 | X_0 = k, X_1 = k-1] \\ 
        = &  p \times \mbb{P} [X_n \in \{1, L\} \text{ for some } n \ge 0 | X_1 = k+1] \\ 
        & + q \times \mbb{P} [X_n \in \{1, L\} \text{ for some } n \ge 0 |  X_1 = k-1] \\ 
        =  &p h_{k+1} + q h_{k-1},
    \end{align*}
    for $k = 2, \cdots, L-1$. And obviously, we have $h_1 = h_L = 1$.
    The characteristic equation for the above linear recurrence relation is 
    $$\lambda = p \lambda^2 + q,$$
    whose solutions are $\lambda_1 = 1, \, \lambda_2 = q/p$. So the general solution to the recursion is 
    $$h_k = c_1 + c_2 \left( \frac{q}{p} \right)^k,$$
    where $c_1, c_2 \in \mbb{R}$ are constants. By the initial conditions $h_1 = h_k = 1$, we konw the coefficients $c_1 =1$ and $c_2 = 0$. Thus, we have $h_k \equiv 1$
\end{example}


\section{Chapman-Kolmogorov Equations}

We have already defined the one-step transition probabilities $P_{i,j}$. We now define the $n$-step transition probabitlies $P^n_{i, j}$.

\begin{definition}
    $P^n_{i,j}$ is defined to be the probability that a homogeneous process in state $i$ will be in state $j$ after $n$ additional transitions. That is,
    $$
    P^n_{i,j} = \mbb{P}[X_{n+k} = j | X_k = i], \qquad n \ge 0,\, i,j \in S.
    $$
\end{definition}

\begin{remark}
    Two simple observations are
    \begin{itemize}
        \item $P^1_{i,j} = P_{i, j}$,
        \item $P^0_{i, j} = \delta(i, j)$.
    \end{itemize}
\end{remark}

\begin{theorem}[Chapman-Kolmogorov Equations]
    The \textbf{Chapman-Kolmogorov equations} provide a method for computing $n$-step transition probabilities. These equations are 
    \begin{equation}\label{C-K Equations}
        P^{n+m}_{i,j} = \sum_{k\in S}P_{i, k}^n P_{k, j}^m \qquad \text{ for all } n,m \ge 0, \text{ and all } i,j \in S.
    \end{equation}

    \begin{proof}
        Formally, we have 
        \begin{align*}
            P^{n+m}_{i,j} = & \mbb{P}[X_{n+m} = j | X_0 = i] \\ 
            = & \sum_{k \in S}\mbb{P} [X_{n+m} = j, X_n = k | X_0 = i] \\ 
            = & \sum_{k \in S} \mbb{P} [X_{n+m} = j | X_n = k, X_0 = i]\mbb{P} [ X_n = k | X_0 = i] \\ 
            = & \sum_{k \in S} \mbb{P} [X_{n+m} = j | X_n = k] \mbb{P}[X_n = k | X_0 = i] \\ 
            =& \sum_{k \in S} P^m_{k, j} P^n_{i, k}.
        \end{align*}
    \end{proof}
\end{theorem}

\begin{corollary}
    If we let $P^{(n)}$ denote the matrix of $n$-step transition probabilities $P_{i,j}^n$, then \eqref{C-K Equations} asserts that 
    $$
    P^{(n+m)} = P^{(n)} P^{(m)}.
    $$
    Hence, in particular, 
    $$
    P^{(2)} = P P = P^2
    $$
    and by induction 
    $$
    P^{(n)} = P^n.
    $$
\end{corollary}

\begin{example} \label{example 4.8}
    Consider Example \ref{example 4.1} in which the weather is considered as a two-state Markov chain. If $\alpha = 0.7$ aans $\beta = 0.4$, then calculate the probability that it will train four days from today given that it is raining today.
    \textit{Sol.} The one-step transition probability matrix is given by 
    $$
    P = 
    \begin{bmatrix}
        0.7 & 0.3 \\ 
        0.4 & 0.6
    \end{bmatrix}
    $$
    Hence, 
    $$
    P^{(2)} = P^2 = 
    \begin{bmatrix}
        0.61 & 0.39 \\ 
        0.52 & 0.48 
    \end{bmatrix},
    $$
    $$
    P^{(4)} = (P^{(2)})^2 = 
    \begin{bmatrix}
        0.5749 & 0.4251 \\ 
        0.5668 & 0.4332
    \end{bmatrix}
    $$
    and the desired probability $P^4_{0,0}$ equals $0.5749$.
\end{example}

\begin{remark}
    Suppose the initial distribution (i.e. the distribution of $X_0$) is given by $\pi_0(x) = \mbb{P} [X_0 = x]$, then the distribution of $X_n$ is then 
    $$
    \pi_n(x) = \sum_{y \in S} \sum_{s_1 \in S} \cdots \sum_{s_{n-1} \in S} \pi_0(y) P_{y, s_1} \cdots P_{s_{n-1}, x}.
    $$
    If we represent the distribution of $X_n$ as a row vector $\mbf{\pi}_n$, then 
    $$
    \mbf{\pi}_n = \mbf{\pi}_0 P^n.$$
\end{remark}

Now consider a discrete-time Markov chain on a finite state space with $|S| = L$, let $\lambda_1, \cdots, \lambda_L \in \mbb{C}$ be the eigenvalues of the transition matrix $P$ with corresponding left (row) eigenvectors $\langle u_i |$ and right (column) eigenvector $|v_i \rangle$, $i = 1, \cdots L$, in bracket notation. Assuming that all eigenvalues are distinct, we have 
$$
P = \sum_{i = 1}^L \lambda_i |v_i\rangle \langle u_i | \quad \text{and} \quad P^n = \sum_{i = 1}^L\lambda_i^n |v_i \rangle \langle u_i,$$
since eigenvectors can be chosen orthonomal $\langle u_i | v_j \rangle = \delta_{i,j}.$

Since $\langle \mbf{\pi}_n | = \langle \mbf{\pi}_0 | P^n$, we get 
$$
\langle \mbf{\pi}_n | = \langle \mbf{\pi}_n | v_1 \rangle \lambda_1^n \langle u_1 | + \cdots + \langle \mbf{\pi}_0 | v_L \rangle \lambda_L^n \langle u_L |.
$$

\begin{itemize}
    \item By the \href{https://en.wikipedia.org/wiki/Gershgorin_circle_theorem}{Gershgorin theorem}, we know there exists at least one eigenvalue $\lambda_i$ such that $|\lambda_i| \le 1$. Contributions with such $\lambda_i$ decay exponentially.
    \item $\lambda_1 = 1$ corresponds to the stationary distribution $\langle \pi | = \langle u_1 |$ and $|v_1 \rangle = \vec{1}$.
    \item Other $\lambda_i \neq 1$ with $|\lambda_1| = 1$ corresponds to persistent oscillations.
\end{itemize}

\begin{definition}[Lazy Versions]
Let $\{X_n : n \in \mbb{N}\}$ be a discrete-time Markov chain with transition matrix $P$. The DTMC with transition matrix
$$
P_{i,j}^\epsilon = \epsilon \delta_{i,j} + (1 - \epsilon) P_{i,j},\, \epsilon \in (0,1)
$$
is called a \textbf{lazy version} of the original chain.
\end{definition}

\begin{itemize}
    \item $P^\epsilon$ has the same eigenvectors as $P$ with eigenvalues $\lambda_i^\epsilon = \lambda_i (1-\epsilon) + \epsilon$ since 
    $$
    \langle u_i | P^\epsilon = \epsilon \langle u_i | + \lambda ( 1 - \epsilon) \langle u_i | \qquad \text{analogously for }|v_i\rangle
    $$
    \item This implies $|\lambda_i^\epsilon| < |\lambda_i| \le 1$ unless $\lambda_i = 1$. Such a matrix $P^\epsilon$ is called \textbf{aperiodic}, and there are no persistent oscillations. (See Section \ref{Limiting Probabilities}.)
    \item The stationary distribution is unique if and only if the eigenvalue $\lambda= 1$ is unique
    (has multiplicity $1$), which is independent of lazyness (discussed later).
\end{itemize}


\section{Classification of States}

\begin{definition}[Accessibility]
    State $j$4 is said to be \textbf{accessible} from state $i$ if $P^n_{i,j} > 0$ for some $n \ge 0$.
\end{definition}

\begin{theorem}
    State $j$ is accessible from state $i$ if and only if, starting in $i$, it is possible that the state will ever enter state $j$. 
    \begin{proof}
        The necessity is implied by the definition of accessibility. Now, suppose the process will ever enter state $j$ from $i$ with a positive probability, but $j$ is not accessible from $i$, then 
        \begin{align*}
            \mbb{P} [\text{ever be in } j | \text{start in } i] = & \mbb{P} \left\{ \bigcup_{n=0}^\infty \{X_n = j\} | X_0 = i \right\} \\ 
            \le & \sum_{n = 0}^\infty \mbb{P} [X_n = j | X_0 = i] \\ 
            = & \sum_{n = 0}^\infty P_{i,j}^n \\ 
            = & 0,
        \end{align*}
        which is a contradiction.
    \end{proof}
\end{theorem}

\begin{definition}[Communication]
    Two states $i$ and $j$ that are accessible to each other are said to \textbf{communicate}, and we write $i \leftrightarrow j$.
\end{definition}

\begin{remark}
    Note that any state communicates with itself, by definition, 
    $$P^0_{i, i} = \mbb{P}[X_n = i | X_n = i] = 1.$$
\end{remark}

\begin{theorem}[Properties of Communication]
    The raltion of communication satisfies the following three properties: 
    \begin{enumerate}
        \item State $i$ communicates with state $i$, $\forall i \in S$.
        \item If state $i$ communicates with state $j$, then state $j$ communicates with state $i$.
        \item If state $i$ communicates with state $j$, and state $j$ communicates with state $k$, then state $i$ communicates with state $k$.
    \end{enumerate}
    \begin{proof}
        Properties $1$ and $2$ follow immediately fromthe definition of communication. To Prove $3$ suppose that $i$ communicates with $j$, and $j$ communicates with $k$. Thus, there exsit interger $n$ and $m$ such that $P^n_{i,j} > 0$, $P^m_{j, k} > 0$. Now by the Chapman-Kolmogorov equations, we have 
        $$
        P^{n+m}_{i, k} = \sum_{r \in S} P^n_{i,r}P^m_{i,r} \ge P^n_{i,j}P^m_{j,k} > 0.
        $$
        Hence, state $k$ is accessible from state $i$. Similarly. we can show that state $i$ is accessible from state $k$. Hence, state $i$ and $k$ communicate.
    \end{proof}
\end{theorem}

\begin{definition}[Classes]
    Two state that communicate are said to be in the same \textbf{class}.
\end{definition}

\begin{remark}
    It is an easy consequence of properties $1$, $2$, $3$ that any two classes of states are either identical or disjoint. In other words, the concept of communication divides the state space up into a number of separate classes.
\end{remark}

\begin{definition}[Irreducibility]
    The Markov chain is said to be \textbf{irreducible} if there is only one class, that is, if all states communicates with each other.
\end{definition}

For any state $i$, we let $f_i$ denote the probability that, starting in state $i$, the process will ever reenter state $i$, i.e.
$$
f_i = \mbb{P} [X_n = i \text{ for some } n > 0 | X_0 = i].
$$

\begin{definition}[Recurrence \& Transience]
    State $i$ is said to be \textbf{recurent} if $f_i = 1$ and \textbf{transient} if $f_i < 1$.
\end{definition}

In Stephan's notes, the difinitions of recurrence and transience are the same as above, but new notations are introduced. 

\begin{definition}[Recurrence \& Transience]
    Let $T_x = \inf\{n \ge 1: X_n = x\}$, then state $x$ is called 
    \begin{itemize}
        \item \textbf{transient}, if $\mbb{P}[T_x = \infty | X_0 = x] > 0$;
        \item \textbf{recurrent}, if $\mbb{P}[T_x < \infty | X_0 = x] = 1$.
    \end{itemize}
\end{definition}

Suppose that the process starts in state $i$ and $i$ is recurent. Hence, with probability $1$, the process will eventually reenter state $i$. However, by the definition of a homogeneous Markov chain, it follows that the process will be starting over again when it reenters state $i$ and, therefore, state $i$ will eventually be visited again. Continual repetition of this argument leads to the following conclusion.

\begin{remark}
    If state $i$ is recurrent then, starting in state $i$, the process will reenter state $i$ again and again and again --- in face, infinitely often.
\end{remark}

On the other hand, suppose that state $i$ is transient. Hence, each time the process enters state $i$ there will be a positive probability, namely, $1 - f_i$, that it will never again enter the state. Therefore, starting in state $i$, the probability that the process will be in state $i$ for exactly $n$ time periods equals $f_i^{n-1}(1 - f_i)$, $n \ge 1$. 

\begin{remark}
    If state $i$ is transient then, starting in state $i$, the number of time periods that the process will be in state $i$ has a \textit{geometric} distribution with finite mean $1/(1-f_i)$.
\end{remark}

From the preceding two paragraphs, it follows that state $i$ is recurrent if and only if, starting in state $i$, the expected number of time periods that the process is in state $i$ is infinite. That is,     Let
$$
I_n = 
\begin{cases}
    1, \qquad &\text{if } X_n = i \\ 
    0, \qquad &\text{if } X_n \neq i
\end{cases}
$$
then state $i$ is recurrent if and only if 
$$
E \left[ \sum_{n = 0}^\infty I_n | X_0 = i\right] = \infty ,
$$
and it is transient if and only if 
$$
E \left[ \sum_{n = 0}^\infty I_n | X_0 = i\right] < \infty.
$$

But one question is how to calculate the expectation of the infinite sum of the indicator variables $I_n$. To get this term, notice that our indicator variables are nonnegative, and using \href{https://en.wikipedia.org/wiki/Fubini%27s_theorem}{Fubini's theorem} for nonnegative functions, we have 
\begin{align*}
    E \left[ \sum_{n = 0}^\infty I_n | X_0 = i\right] = & \sum_{n=0}^\infty E[ I_n | X_0 =i ] \\ 
    = & \sum_{n=0}^\infty \mbb{P}[X_n = i | X_0 = i] \\ 
    = & \sum_{n=0}^\infty P_{i, i}^n.
\end{align*}

\begin{proposition}\label{Recurrence and Transience prop}
    For a homogeneous discrete-time Markov chain with its state space $S$ and transition matrix $P$, for any state $i \in S$, $i$ is 
    recurrent if and only if 
    $$
    \sum_{n=0}^\infty P_{i, i}^n = \infty,
    $$
    and transient if and only if 
    $$
    \sum_{n=0}^\infty P_{i, i}^n < \infty.
    $$
\end{proposition}

The argument leading to the proceding proposition is doubly important because it also shows that a transient state will only be visited a finite number of times (hence the name transient). This leads to the conclusion that \textit{in a finite-state Markov chain not, not all states can be transient}. 

To see this, suppose the states are $0, 1, \cdots, M$ and suppose that they are all transient. Then after a finite amount of time (say, after time $T_0$) state $0$ will never be visited, and after a time (say $T_1$) state $1$ will never be visited, and after a time (say $T_2$) state $2$ will never be visited, and so on. Thus, after a finite time $T = \max \{T_0, T_1, \cdots, T_M\}$ no state will be visited. But as the process must be in some states after time $T$ we arrive at a contradiction, which shows that at leasat one of the states must be recurrent.

Another use of Proposition \ref{Recurrence and Transience prop} is that it enables us to show that recurrence is a class property.

\begin{corollary}\label{recurrence is a class property}
    If state $i$ is recurrent, and state $i$ communicates with state $j$, then state $j$ is recurrent.
    \begin{proof}
        To prove this we first note that, since state $i$ communicates with state $j$, there exist intergers $k$ and $m$ such that $P^k_{i,j} > 0$, $P^m_{j,i} > 0$. Now for any integer $n$ 
        $$
        P^{m+n+k} _{j,j} \ge P^m_{j,i} P^n_{i,i} P^k_{i,j}.
        $$
        This follows since the left side of the preceding is the probability of going from $j$ to $j$ in $m+n+k$ steps, while the right side is the probability of going from $j$ to $j$ in $m_n+k$ steps via a path that goes from $j$ to $i$ in $m$ steps, then from $i$ to $i$ in an additional steps, then from $i$ to $j$ in an additional $k$ steps.
        
        From the preceding we obtain, by summing over $n$, that 
        $$
        \sum_{n = 1}^\infty P_{j,j}^{n} \ge \sum_{n = 1}^\infty P_{j,j}^{m+n+k} \ge P^{m}_{j,i}P^k_{i,j} \sum_{n = 1}^\infty P_{j,j}^{n} = \infty 
        $$
        since $P^{m}_{j,i}P^k_{i,j} > 0$ and $\sum_{n = 1}^\infty P_{j,j}^{n} $ is infinite since state $i$ is recurrent. Thus, by Proposition \ref{Recurrence and Transience prop} it follows that state $j$ is also recurrent.
    \end{proof}
\end{corollary}

\begin{remark}
    \begin{enumerate}
        \item Corollary \ref{recurrence is a class property} also implies that \textit{transience is a class propertya}. For if state $i$ is transient and communicates with state $j$ , then state $j$ must also be transient. For if $j$ were recurrent then, by Corollary \ref{recurrence is a class property}, $i$ would also be recurrent and hence could not be transient.
        \item Corollary \ref{recurrence is a class property} along with our previous result that not all states in a finite Markov chain can be transient leads to the conclusion that \textit{all states of a finite irreducible Markov chain are recurrent.}
    \end{enumerate}
\end{remark}

\section{Long-Run Proportions and Limiting Probabilities}

For pairs of states $i \neq j$, let $f_{i, j}$ denote the probability that the Markov chain, starting in state $i$, will ever make a transition into state $j$. That is,
$$
f_{i,j} = \mbb{P} [X_n = j \text{ for some } n > 0 | X_0 = i].
$$
We then have the following result.
\begin{proposition}\label{Recurrence & Communication}
    If $i$ is recurrent and $i$ communicates with $j$, then $f_{i,j} = 1$
    .
    \begin{proof}
        Because $i$ and $j$ communicate there is a value $n$ such that $P_{i,j}^n > 0$. Let $X_0 = i$ and say that the first opportunity is a success if $X_n = j$, and note that the first opportunity is a success with probability $P^n_{i,j} > 0$. If the first opportunity is not a success, then consider the next time (after time $n$) that the chain enters state $i$. (Because state $i$ is recurrent we can be certain that it will eventually reenter state $i$ after time $n$.) Say that the second opportunity is a success if $n$ time periods later the Markov chain is in state $j$ . If the second opportunity is not a success then wait until the next time the chain enters state $i$ and say that the third opportunity is a success if $n$ time periods later the Markov chain is in state $j$.
        
        Continuing in this manner, we can define an unlimited number of opportunities, each of which is a success with the same positive probability $P^n_{i,j}$. Because the number of opportunities until the first success occurs is geometric with parameter $P^n_{i,j}$, it follows that with probability $1$ a success will eventually occur and so, with probability $1$, state $j$ will eventually be entered.
    \end{proof}
\end{proposition}

If state $j$ is recurrent, let $m_j$ denote the expected number of transitions that it takes the Markov chain when starting in state $j$ to return to that state. That is, with
$$
N_j = \min\{n > 0 : X_n = j \}
$$
equal to the number of transitions until the Markov chain makes a transition into
state $j$ ,
$$
m_j = \mbb{E}[N_j |X_0 = j ].
$$

\begin{remark}
    In Stephan's notes, there is a similar notation to $N_j$, which is called $T_x$ and defined by 
    $$T_x = \inf \{n > 0: T_n = x\}.$$
    $T_x$ is called the \textbf{first return time} to state $x$. 

    If state $j$ is recurrent, then these two notations are equivalent. Otherwise, $N_j$ may not exist and $T_j$ in this case will be $\infty$.
\end{remark}

\begin{definition}[Positive Recurrence \& Null Recurrence]
    Say that the recurrent state $j$ is \textbf{positive recurent} if $m_j < \infty$ and say that it is \textbf{null recurrent} if $m_j = \infty$.
\end{definition}

Stephan's definitions are similar. 

\begin{definition}[Transient \& Positive Recurrence \& Null Recurrence]
    Let $T_x$ be the \textbf{first return time} to state $x$ (i.e. $T_x = \inf \{ n > 0: X_n = x\}$). A state $x \in S$ is called 
    \begin{itemize}
        \item \textbf{transient}, if $\mbb{P}[T_x = \infty] > 0$;
        \item \textbf{positive recurent}, if $\mbb{P} [T_x < \infty] = 1$ and $\mbb{E} [T_x | X_0 = x] < \infty$;
        \item \textbf{null recurent}, if $\mbb{P} [T_x < \infty] = 1$ and $\mbb{E} [T_x | X_0 = x] = \infty$.
    \end{itemize}
\end{definition}

Now suppose that the Markov chain is irreducible and recurrent. In this case we now show that \textit{the long-run proportion of time that the chain spends in state $j$ is equal to $1/m_j$.} That is, letting $\pi_j$ denote the \textbf{long-run proportion of time} that the Markov chain is in state $j$ , we have the following proposition.

\begin{proposition}
    If the Markov chain is irreducible and recurrent, then for any initial state
    $$
    \pi_j = 1/m_j.
    $$
    \begin{proof}
        Suppose that the Markov chain starts in state $i$, and let $T_1$ denote the number of transitions until the chain enters state $j$; then let $T_2$ denote the additional number of transitions from time $T_1$ until the Markov chain next enters state $j$; then let $T_3$ denote the additional number of transitions from time $T_1 + T_2$ until the Markov chain next enters state $j$, and so on. Note that $T_1$ is finite because Proposition \ref{Recurrence & Communication} tells us that with probability $1$ a transition into $j$ will eventually occur. Also, for $n \ge 2$, because $T_n$ is the number of transitions between the $(n − 1)$th and the $n$th transition into state $j$ , it follows from the Markovian property that $T_2$ , $T_3$ , $\cdots$ are independent and identically distributed with mean $m_j$. Because the $n$th transition into state $j$ occurs at time $T_1 + \cdots + T_n$ we obtain that $\pi_j$ , the long-run proportion of time that the chain is in state $j$, is 
        \begin{align*}
            \pi_j = & \lim_{n\to\infty} \frac{n}{\sum_{i=1}^n T_i} \\ 
            = & \lim_{n\to\infty} \frac{1}{\frac{1}{n}\sum_{i=1}^n T_i} \\ 
            = & \lim_{n\to\infty} \frac{1}{\frac{T_1}{n} + \frac{T_2 + \cdots + T_n}{n}} \\ 
            = & \frac{1}{m_j},
        \end{align*}
        where the last inequality follows because $\lim_{n\to\infty} T_1/n = 0$ and, from the strong law of large numbers, $\lim_{n\to\infty} \frac{T_2 + \cdots + T_n}{n} = \lim_{n \to \infty} \frac{T_2 + \cdots + T_n}{n-1} \frac{n-1}{n} = m_j$.
    \end{proof}
\end{proposition}

\begin{remark}
    Because $m_j < \infty$ is equivalent to $1/m_j > 0$, it follows from the preceding that state $j$ is positive recurrent if and only if $\pi_j > 0$. 
\end{remark}

We now exploit the above remark to show that positive recurrence is a class property.

\begin{proposition}\label{positive recurrence is a class property}
    If $i$ is positive recurrent and $i \leftrightarrow j$ then $j$ is positive recurrent.
    \begin{proof}
        Suppose that $i$ is positive recurrent and that $i \leftrightarrow j$ . Now, let $n$ be such that $P^n_{i,j}> 0$. Because $\pi_i$ is the long-run proportion of time that the chain is in state $i$, and $P^n_{i,j}$ is the probability that it will be in state $j$ after $n$ transitions from $i$.
        \begin{align*}
            \pi_i P^n_{i,j} = & \text{long-run proportion of time the chain is in $i$} \\ 
            &\text{and will be in $j$ after $n$ transitions} \\ 
            = & \text{long-run proportion of time the chain is in $j$}\\
            & \text{and was in $i$ $n$ transitions ago} \\ 
            \le & \text{long-run proportion of time the chain is in $j$}.
        \end{align*}
        Hence, $\pi_j \ge \pi_i P^n_{i,j} > 0$, showing that $j$ is positive recurrent.
    \end{proof}
\end{proposition}

\begin{remark}
    \begin{enumerate}
        \item It follows from the preceding result that \textit{null recurrence is also a class property.} For suppose that $i$ is null recurrent and $i \leftrightarrow j$. Because $i$ is recurrent and $i \leftrightarrow j$ we can conclude that $j$ is recurrent. But if $j$ were positive recurrent then by the preceding proposition $i$ would also be positive recurrent.
        Because $i$ is not positive recurrent, neither is $j$.
        \item \textit{An irreducible finite state Markov chain must be positive recurrent.} For we know that such a chain must be recurrent; hence, all its states are either positive recurrent or null recurrent. If they were null recurrent then all the long run proportions would equal $0$, which is impossible when there are only a finite number of states. Consequently, we can conclude that the chain is positive recurrent.
        \item The classical example of a null recurrent Markov chain is the one dimensional symmetric random walk.
    \end{enumerate}
\end{remark}

To determine the long-run proportions $\{\pi_j , j \ge 1\}$, note, because $\pi_i$ is the long-run proportion of transitions that come from state $i$, that
$$
\pi_i P_{i,j} = \text{long-run proportion of transitions that go from state $i$ to state $j$},
$$
summing the preceding over all $i$ now yields that
$$
\pi_j = \sum_{i \in S}\pi_i P_{i,j}.
$$
Indeed, the following important theorem can be proven.

\begin{theorem}\label{stationary distribution}
    Consider an irreducible Markov chain. If the chain is positive recurrent, then the long-run proportions are the unique solution of the equations
    \begin{align*}
        \pi_j & = \sum_{i \in S} \pi_i P_{i, j} \\ 
        \sum_{j \in S} \pi_j & = 1
    \end{align*}
    Moreover, if there is no solution of the preceding linear equations, then the Markov chain is either transient or null recurrent and all $\pi_j = 0$.
\end{theorem}



\begin{example}\label{example 4.22}
    Consider Example \ref{example 4.1}, in which we assume that if it rains today, then
    it will rain tomorrow with probability α; and if it does not rain today, then it will rain tomorrow with probability $\beta$. If we say that the state is $0$ when it rains and $1$ when it
    does not rain, then by Theorem \ref{stationary distribution} the long-run proportions $\pi_0$ and $\pi_1$ are given by
    \begin{align*}
        \pi_0 = & \alpha \pi_0 + \beta \pi_1, \\ 
        \pi_1 = & (1- \alpha)\pi_0 + (1 - \beta) \pi_1, \\ 
        \pi_0 + \pi_1 = & 1,
    \end{align*}
    which yields that 
    $$
    \pi_0 = \frac{\beta}{1+\beta - \alpha}, \, \pi_1 = \frac{1 - \alpha}{1 + \beta - \alpha}.
    $$
    For example, if $\alpha = 0.7$ and $\beta = 0.4$, then the long-run proportion of rain is $\pi_0 = \frac{4}{7} = 0.571$.
\end{example}

\begin{proposition}
    If the initial state is chosen according to the probabilities $\pi_j$, then the probability of being in state $j$ at any time $n$ is also equal to $\pi_j$. That is, if 
    $$
    \mbb{P} [X_0 = j] = \pi_j, \, \forall j \in S,
    $$
    then 
    $$
    \mbb{P} [X_n = j] = \pi_j, \, \forall n \ge 0.
    $$
    \begin{proof}
        This can be easily proven by induction, for it is true when $n = 0$, and if we suppose it is true for $n-1$, then writing 
        \begin{align*}
            \mbb{P}[X_n = j] = & \sum_{i \in S} \mbb{P} [X_n = j | X_{n-1} = i] \mbb{P} [X_{n-1} = i] \\ 
            = & \sum_{i \in S} P_{i,j} \pi_i \qquad \text{by induction hypothesis} \\ 
            = & \pi_j \qquad \text{by Theorem \ref{stationary distribution}}
        \end{align*}
    \end{proof}
\end{proposition}

\begin{definition}[Stationary Probabilities]
    The long-run propoertions $\pi_j$, are aften called \textbf{stationary probabilities}.
\end{definition}

\begin{definition}[Stationary MC]
    A MC is \textbf{stationary} if it has stationary probabilitiy $\mbf{\pi}$.
\end{definition}

\begin{theorem}[Existence of Stationary Probabilities]
    \label{existence of stationary probabilities}
    Every homogeneous discrete-time MC with a finite state space has a stationary probability distribution.
    \begin{proof}
        Let $\Delta = \{\text{probabilities } \mbf{\pi} \text{ on the MC } | \mbf{\pi} \vec{1} = 1, \mbf{\pi}_i \ge 0\}$. Since $P_{i,j} \ge 0$ and $P \vec{1} = \vec{1}$, we have $\mbf{\pi} \in \Delta \implies \mbf{\pi} P \in \Delta$.

        Notice that $\Delta$ is compact (which is equivalently closed and bounded, since it is in a finite linear space) and convex, and $P$ is continuous (actually it is linear). By \href{https://en.wikipedia.org/wiki/Brouwer_fixed-point_theorem}{Brouwer fixed-point theorem}, $P$ has a fixed point $\mbf{\pi}^*$ in $\Delta$, i.e. $\mbf{\pi}^* P = \mbf{\pi}^*$.
    \end{proof}
\end{theorem}

\begin{remark}
    The uniqueness of stationary distributions is not guaranteed. And note that any convex combination of two stationary distributions is also a stationary distributions.
\end{remark}

The proof of Theorem \ref{existence of stationary probabilities} is not constructive, and an even better way is to prove that the mean fraction of time spent in each state from a given initial state is a stationary probability. But we need first a decomposition theorem for homogeneous finite discrete-time Markov chain. 

\begin{definition}[Cycles]
    A \textbf{cycle} is a closed path (or walk) in $S$ along the graph of allowed transitions (defined by $P$) of length greater than $0$.
\end{definition}

Robert used the following definitions for transience and recurrence. which are equivalent to those previously defined but more practical.
\begin{definition}[Transience \& Recurrence]
    Say $i \in X$ is \textbf{transient} if $\nexists$ cycles throught $i$. Otherwise, $i$ is \textbf{recurrent}.
\end{definition}

\begin{definition}[Communication]
    Say $i,j$ \textbf{communicates}, and denote this as $i \leftrightarrow j$, if there exists a cycle through $i$ and $j$.
\end{definition}

\begin{proposition}
    Communication is an equivalent relation on the set of recurrent states:
    \begin{itemize}
        \item $i \leftrightarrow i$;
        \item $i \leftrightarrow j \iff j \leftrightarrow i$;
        \item $i \leftrightarrow j,\, j \leftrightarrow k \implies i \leftrightarrow k$.
    \end{itemize}
\end{proposition}

\begin{definition}[Communicating Components]
    Equivalent classes are called \textbf{communicating components}.
\end{definition}

\begin{definition}[Absorbing Communicating Components]
    A communicating componen is \textbf{absorbing} if it is impossible to leave it.
\end{definition}

The transition graph for a MC can be quotiented by communication relations, and the resulting graph is acyclic (DAG). The basal communicating components are absorbing ones. 

Finiteness of a MC indicates that there exists an absorbing communicating component. We can reduce a MC to an absorbing component, and the result is called an \textbf{irreducible} MC.

Let $x \in A$, where $A$ is an absorbing component, and let $m_x$ be the mean time to return to $x$ conditional on starting at $x$. Finiteness implies $m_x < \infty$.

For an arbitrary $y \in S$, let $\gamma_x(y)$ be the mean time in $y$ before return to $x$ given that the process starts at $x$. Notice that $\gamma_x(y) \ge 0$ and $\gamma_x(y) = 0$ if $y \notin A$. Then $\gamma_x P = \gamma_x$. (Use $\mbb{E} [\mbb{E} [X|Y]] = \mbb{E}[X]$.) Let $\mbf{\pi}_x(y) = \gamma_x(y) / m_x$, then $\mbf{\pi}_x \vec{1} = 1$. So $\mbf{\pi}_x$ is a stationary probability.

\begin{remark}
    Next, $\mbf{\pi}_x = \mbf{\pi}_y$ if and only if $x,\,y$ are in the same absorbing component $A$, so denote it by $\mbf{\pi}_A$.
\end{remark}

\begin{remark}
    Also, 
$$
\mbf{\pi}_A = 
\begin{cases}
    \frac{1}{m_x} \qquad &\text{for } x \in A \\ 
    0 \qquad &\text{for } x \notin A
\end{cases}
$$
\end{remark}


Furthermore, $\mbf{\pi}_A$ is DS-ergodic.

\begin{definition}[Dynamical-System Ergodicty]
    A stationary probability is \textbf{dynamical-system ergodic} if it is not a convex combination of other stationary probabilities.
\end{definition}

\begin{theorem}[DS-Ergodic Theorem]
    If $\mbf{\pi}$ is DS-ergodic, then $\forall x \in S$ with $\mbf{\pi}(x) >0$, the fraction of times $0, \cdots, T-1$ spent in any $y \in S$ given that the process starts in $x$ converges almost surely to $\mbf{\pi}(y)$ as $T \to \infty$.
\end{theorem}

\begin{remark}
    If a \textit{finite discrete-time} MC has a \textit{unique absorbing component $A$}, then the fraction of time spent in $y$ given that the process starts anywhere converges \textit{almost surely} to $\mbf{\pi}_A (y)$ and $\mbf{\pi}_A$ is the \textit{only} stationary probability. DS theorists would say the MC is \textbf{uniquely ergodic}.
\end{remark}

\begin{remark}
    If the MC has more than one absorbing components $A_j$, then there are "commutor probabilities" $C_{z,A_j}$ for which absorbing component $A_j$ you eventually land in given that starts in $z \in S$. Then the fraction of time spent in $y$ converges to $\mbf{\pi}_{A_j}$ with probability $C_{z, A_j}$, as $T \to \infty$.
\end{remark}




\subsection{Limiting Probabilities}\label{Limiting Probabilities}

In Example \ref{example 4.8} we considered a two-state Markov chain with transition probability matrix
$$
P = 
\begin{bmatrix}
    0.7 & 0.3 \\ 
    0.4 & 0.6
\end{bmatrix}
$$
and showed that 
$$
P^{(4)} = 
\begin{bmatrix}
    0.5749 & 0.4251 \\ 
    0.5668 & 0.4332
\end{bmatrix}.
$$
From this it follows that $P^{(8)} = P^{(4)}  P^{(4)} $ is given by 
$$
P^{(8)} = 
\begin{bmatrix}
    0.571 & 0.429 \\ 
    0.571 & 0.429 
\end{bmatrix}.
$$
Note that the matrix $P^{(8)}$ is almost identical to the matrix $P^{(4)}$, and that each of the
rows of $P^{(8)}$ has almost identical values. Indeed, it seems that $P_{i,j}^n$ is converging to
some value as $n \to \infty$, with this value not depending on $i$. Moreover, in Example \ref{example 4.22}
we showed that the long-run proportions for this chain are $\pi_0 = 4/7 ≈ 0.571,\, \pi_1 = 3/7 ≈ 0.429$, thus making it appear that these long-run proportions may also be limiting probabilities. Although this is indeed the case for the preceding chain, it is not
always true that the long-run proportions are also limiting probabilities. To see why
not, consider a two-state Markov chain having
$$
P_{0,1} = P_{1,0} = 1.
$$
Because this Markov chain continually alternates between states $0$ and $1$, the long-run
proportions of time it spends in these states are
$$
\pi_0 = \pi_1 = 1/2.$$
However, 
$$
P^n_{0,0} = \begin{cases}
    1, \qquad &\text{if $n$ is even} \\ 
    0, \qquad &\text{if $n$ is odd}
\end{cases},
$$
and so $P^n_{0,0}$ does not have a limiting value as $n$ goes to infinity. 

\begin{definition}[Periodicity]
    In general, a chain that
    can only return to a state in a multiple of $d > 1$ steps (where $d = 2$ in the preceding
    example) is said to be \textbf{periodic} and does not have limiting probabilities. 
\end{definition}

\begin{proposition}
    For an irreducible chain that is not periodic, and such chains are called \textbf{aperiodic},
    the limiting probabilities will always exist and will not depend on the initial state.
    Moreover, the limiting probability that the chain will be in state $j$ will equal $\pi_j$, the long-run proportion of time the chain is in state $j$.

    \begin{proof}
        That the limiting probabilities,
        when they exist, will equal the long-run proportions can be seen by letting
        $$
        \alpha_j = \lim_{n\to\infty} \mbb{P} [X_n = j],
        $$
        and using that 
        $$
        \mbb{P} [ X_{n+1} = j ] = \sum_{i \in S} \mbb{P} [X_{n+1} = j | X_n = i] \mbb{P} [X_n = i] = \sum_{i \in S} P_{i,j} \mbb{P})[X_n = i]
        $$
        and 
        $$
        1 = \sum_{i \in S} \mbb{P} [ X_n = i].
        $$
        Letting $n \to \infty$ in the preceding two equations yields, upon assuming that we can bring the limit inside the summation, that
        \begin{align*}
            \alpha_j = & \sum_{i \in S} \alpha_i P_{i,j}  \\
            1 = & \sum_{i \in S} \alpha_i
        \end{align*}
        Hence, $\{\alpha_j , j \ge 0\}$ satisfies the equations for which $\{\pi_j , j \ge 0\}$ is the unique solution, showing that $\alpha_j = \pi_j, j \ge 0$.
    \end{proof}
\end{proposition}

\begin{definition}[Ergodicity]
    An irreducible, positive recurrent, aperiodic Markov chain is said to be \textbf{ergodic}. 
\end{definition}


Robert talked about the stochastic-process ergodicity in the case that the MC has a finite state space. 

\begin{definition}[Stochastic-Process Ergodicity]
    Let $\{X_n: n \in \mbb{N}\}$ be a Markov chain with transition matrix $P$. If $\exists \, \mbf{\pi} \in \Delta$, where $\Delta = \{ \mbf{x} \in \mbb{R}^{1 \times |S|}| \mbf{x} \vec{1} = 1, \mbf{x}_i \ge 0, i = 1, \cdots |S|\}$, such that $\forall \, \mbf{\pi}_0 \in \Delta, \mbf{\pi}_0 P^n \to \mbf{\pi}$, as $n \to \infty$, then the MC is \textbf{stochastic-process ergodic}. 
\end{definition}

\begin{remark}
    All senses of convergence are equivalent in finite state space, but let's say in $l_1$.
\end{remark}

\begin{remark}
    This tells us not only that the fraction of time spent in state $y$ converges almost surely to $\mbf{\pi}_y$ for any initial condition, but also that the probability of $X_n = y$ converges to $\mbf{\pi}_y$ as $n \to \infty$.
\end{remark}

\begin{remark}
    There are finite discrete-time MCs with a unique absorbing component (so dynamical-system ergodic) which are not stochastic-process ergodic. The obstruction is periodicity. The two-state MC discussed at the beginnig of this section is an example.
\end{remark}

\begin{theorem}
    A homogeneous finite discrete-time Markov chain is stochastic-process ergodic if it has a unique absorbing component and it is aperiodic.
    \begin{proof}
        The proof is by the \href{https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem}{Perro-Frobenius theorem}. For an irreducible, aperiodic, non-negative matrix, there is a unique and simple eigenvalue of maximum modulus, and it has a positive eigenvector. 

        In our case, the largest modulus of eigenvalue $P$ can have is $1$ by conservation of probability (or the Gershgorin theorem).
        And we konw it has eigenvalue $1$, since $P \vec{1} = \vec{1}$.

        So we can restrict the MC to the unique absorbing component, and thus, $1$ is simple and has a positive left eigenvector $\mbf{\pi}$ which we normalize to $\mbf{\pi} \vec{1} = 1$. Then $\mbf{\pi}_0 P^n = a \mbf{\pi} + \text{ terms going to }0 \text{ as }n \to \infty$, and $\mbf{\pi}_0 \vec{1} = 1$, $P \vec{1} = \vec{1}$, so $a = 1$. Hence the MC is SP-ergodic.
    \end{proof}
\end{theorem}


\section{Mean Time Spent in Transient States}

Consider now a finite-state Markov chain and suppose that the states are numbered so that $T = \{1, 2, \cdots , t\}$ denotes the set of transient states. Let

$$
P_T = \begin{bmatrix}
    P_{1,1} & P_{1,2} & \cdots & P_{1, t} \\ 
    P_{2,1} & P_{2,2} & \cdots & P_{2, t} \\ 
    \vdots & \vdots & \ddots & \vdots \\ 
    P_{n,1} & P_{n,2} & \cdots & P_{t,t}
\end{bmatrix}
$$

and note that since $P_T$ specifies only the transition probabilities from transient states into transient states, some of its row sums are less than $1$ (otherwise, $T$ would be a closed class of states). For transient states $i$ and $j$ , let $s_{i,j}$ denote the expected number of time periods that the Markov chain is in state $j$ , given that it starts in state $i$. Let $\delta_{i,j} = 1$ when $i = j$, and let it be $0$ otherwise. Condition on the initial transition to obtain

\begin{align}
    s_{i,j} = & \delta_{i,j} + \sum_{k \in S} P_{i,k} s_{k,j} \notag \\ 
    = & \delta_{i,j} + \sum_{k=1}^t P_{i,k} s_{k,j} \label{s elementwise}
\end{align}

where the final equality follows since it is impossible to go from a recurrent to a transient state, implying that $s_{k,j} = 0$ when k is a recurrent state.

Let $S$ denote the matrix of values $s_{i,j} , i, j = 1, \cdots , t$. That is,

$$
S = \begin{bmatrix}
    s_{1,1} & s_{1,2} & \cdots & s_{1, t} \\ 
    s_{2,1} & s_{2,2} & \cdots & s_{2, t} \\ 
    \vdots & \vdots & \ddots & \vdots \\ 
    s_{n,1} & s_{n,2} & \cdots & s_{t,t}
\end{bmatrix}
$$

In matrix notation, Eq. \eqref{s elementwise} can be written as

$$
S = I + P_T S.
$$

where $I$ is the identity matrix of size $t$. Because the preceding equation is equivalent to

$$
(I - P_T)S = I
$$

we obtain, upon multiplying both sides by $(I − P_T )^{−1}$ ,
$$
S = (I - P_T )^{-1}.
$$

That is, the quantities $s_{i,j}$, $i \in T$ , $j \in T$ , can be obtained by inverting the matrix $I - P_T$.
(The existence of the inverse is established on the fact that $I-P_T$ is diagonally dominant.)

For $i \in T$ , $j \in T$, the quantity $f_{i,j}$, equal to the probability that the Markov chain ever makes a transition into state $j$ given that it starts in state $i$, is easily determined from $P_T$. To determine the relationship, let us start by deriving an expression for $s_{i,j}$ by conditioning on whether state $j$ is ever entered. This yields

\begin{align*}
    s_{i,j} = & \mbb{E} [\text{time in } j |\text{start in } i, \text{ ever transit to } j ] f_{i,j} \\ 
    & + \mbb{E} [\text{time in } j |\text{start in } i, \text{ never transit to } j ](1 - f_{i,j}) \\ 
    = & (\delta_{i,j} + s_{j,j}) f_{i,j} + \delta_{i,j} (1 - f_{i,j}) \\ 
    = & \delta_{i,j} + s_{j,j} f_{i,j}.
\end{align*}

Solving the preceding equation yields
$$
f_{i,j} = \frac{s_{i,j} - \delta_{i,j}}{s_{j,j}}.
$$

Suppose we are interested in the expected time until the Markov chain enters some sets of states $A$, which need not be the set of recurrent states. We can reduce this back to the previous situation by making all states in $A$ absorbing states. That is, reset the transition probabilities of states in $A$ to satisfy 
$$P_{i,i} = 1,\, i \in A.$$
This transforms the states of $A$ into recurrent states, and transforms any state outside of $A$ from which an eventual transition into $A$ is possible into a transient state. Thus, our previous approach can be used.




\section{Time Reversible Markov Chains}

Consider a stationary ergodic Markov chain (that is, an ergodic Markov chain that has
been in operation for a long time) having transition probabilities $P_{i,j}$ and stationary
probabilities $\pi_i$ , and suppose that starting at some time we trace the sequence of states going backward in time. That is, starting at time $n$, consider the sequence of states $X_n , X_{n-1} , X_{n-2} , \cdots$. It turns out that this sequence of states is itself a Markov chain
with transition probabilities $Q_{i,j}$ defined by

\begin{align*}
    Q_{i,j} = & \mbb{P} [X_m = j | X_{m+1} = i] \\ 
    = & \frac{\mbb{P} [X_m = j, X_{m+1} = i]}{\mbb{P} [X_{m+1} = i]} \\ 
    = & \frac{\mbb{P}[X_m = j] \mbb{P} [X_{m+1} = i | X_m = j]}{\mbb{P} [X_{m+1} = i]} \\ 
    = & \frac{\pi_j P_{j,i}}{\pi_i}
\end{align*}

To prove that the reversed process is indeed a Markov chain, we must verify that
$$
\mbb{P} [X_m = j | X_{m+1} = i, X_{m+2} = s_{m+2}, X_{m+3} = s_{m+3}, \cdots] = \mbb{P} [X_m = j | X_{m+1} = i].$$

To see that this is so, suppose that the present time is $m + 1$. Now, since $X_0 , X_1 , X_2 , \cdots$
is a Markov chain, it follows that the conditional distribution of the future $X_{m+2} ,
X_{m+3} , \cdots$ given the present state $X_{m+1}$ is independent of the past state $X_m$ . However, independence is a symmetric relationship (that is, if $A$ is independent of $B$, then
$B$ is independent of $A$), and so this means that given $X_{m+1}$, $X_{m}$ is independent of
$X_{m+2} , X_{m+3} , \cdots$. But this is exactly what we had to verify.

Thus, the reversed process is also a Markov chain with transition probabilities given
$$
Q_{i,j} = \frac{\pi_j P_{j,i}}{\pi_i}.$$

\begin{remark}
    In general, the reverse chain is not homogeneous even if the original MC is, but if the MC is stationary with stationary probability $\mbf{\pi}$, then 
    $$
    Q_{i,j} = \frac{\pi_j}{\pi_i}P_{i,j}.
    $$
    So the reverse MC is also homogeneous and stationary with the same probability $\mbf{\pi}$.
\end{remark}

\begin{definition}[Time Reversibility]
    If $Q_{i,j} = P_{i,j}$ for all $i,j$, then the Markov chain is said to be \textbf{time reversible}. 
\end{definition}

\begin{definition}[Detailed Balance Conditions]
    The condition for time reversibility, namely, $Q_{i,j} = P_{i,j}$ can also be expressed as 
    \begin{equation}
    \pi_i P_{i,j} = \pi_j P_{j, i} \qquad \text{for all } i, j \label{eq 4.21}
    \end{equation}
    Thesse conditions are called \textbf{detailed balance conditions}.
\end{definition}


The condition in Eq. \eqref{eq 4.21} can be stated that, for all states $i$ and $j$ , the rate at which the process goes from $i$ to $j$ (namely, $\pi_i P_{i,j}$ ) is equal to the rate at which it goes from $j$
to $i$ (namely, $\pi_j P_{j,i}$ ). It is worth noting that this is an obvious necessary condition for
time reversibility since a transition from $i$ to $j$ going backward in time is equivalent to
a transition from $j$ to $i$ going forward in time; that is, if $X_m = i$ and $X_{m−1} = j$ , then
a transition from $i$ to $j$ is observed if we are looking backward, and one from $j$ to $i$ if
we are looking forward in time. Thus, the rate at which the forward process makes a transition from $j$ to $i$ is always equal to the rate at which the reverse process makes a transition from $i$ to $j$; if time reversible, this must equal the rate at which the forward
process makes a transition from $i$ to $j$.

\begin{proposition}
    If we can find nonnegative numbers, summing to one, that satisfy Eq. \eqref{eq 4.21}, then it follows that the Markov chain is time reversible and the numbers represent the limiting probabilities.
    \begin{proof}
        This is so since if
        \begin{equation}\label{eq 4.22}
            x_i P_{i,j} = x_j P_{j,i} \qquad \text{for all $i$, $j$,} 
            \sum_{i \in S} x_i = 1 ,
        \end{equation}
        then summing over $i$ yields 
        $$
        \sum_{i\in S} x_i P_{i,j} = x_j \sum_{i \in S} P_{j, i} = x_j, \, \sum_{i \in S} x_i = 1,$$
        and, because the limiting probabilities $\pi_i$ are the unique solution of the preceding, it follows that $x_i = \pi_i$ for all $i$.
    \end{proof}
\end{proposition}

If we try to solve Eq. \eqref{eq 4.22} for an arbitrary Markov chain with states $0, 1, \cdots , M$,
it will usually turn out that no solution exists. For example, from Eq. \eqref{eq 4.22}, implying (if $P_{i,j}P_{j,k} > 0$) that
$$
\frac{x_i}{x_k} = \frac{P_{j,i}P_{k,j}}{P_{i,j}P_{j,k}}
$$
which in general need not equal $P_{k,i}/P_{i,k}$. Thus, we see that a necessary condition for time reversibility is that 
\begin{equation}\label{eq 4.25}
P_{i,k} P_{k,j} P_{j,i} = P_{i,j} P_{j,k} P_{k,i} \quad \text{for all $i,j,k$}
\end{equation}
which is equivalent to the statement that, starting in state $i$, the path $i \to k \to j \to i$
has the same probability as the reversed path $i \to j \to k \to i$. To understand the
necessity of this, note that time reversibility implies that the rate at which a sequence
of transitions from $i$ to $k$ to $j$ to $i$ occurs must equal the rate of ones from $i$ to $j$ to $k$
to $i$, and so we must have
$$
\pi_i P_{i,k} P_{k,j} P_{j,i} = \pi_i P_{i,j} P_{j,k} P_{k,i}
$$
implying Eq. \eqref{eq 4.25} when $\pi_i > 0$.

In fact, we have the following

\begin{theorem}
    A stationary Markov chain for which $P_{i,j} = 0$ whenever $P_{j,i} = 0$ is time reversible if and only if starting in state $i$, any path back to $i$ has the same probability as the reversed path. That is, if
    \begin{equation}\label{eq 4.26}
        P_{i,i_1}P_{i_1,i_2}\cdots P_{i_k, i} = P_{i,i_k} P_{i_k, i_k-1} \cdots P_{i_1, i}
    \end{equation}
    for all states $i, i_1, \cdots, i_k$.
    \begin{proof}
        We have already proven necessity. To prove sufficiency, fix states $i$ and $j$ and
        rewrite Eq. \eqref{eq 4.26} as
        $$
        P_{i,i_1} P_{i_1 ,i_2} \cdots  P_{i_k ,j} P_{j,i} = P_{i,j} P_{j,i_k} \cdots P_{i_1 ,i}
        $$
        Summing the preceding over all states $i_1 , \cdots , i_k$ yields
        $$
        P_{i,j}^{k+1} P_{j,i} = P_{i,j} P_{j,i}^{k+1}
        $$
        Consequently,
        $$
        \frac{P_{j,i}\sum_{k = 1}^m P^{k+1}_{i,j}}{m} = \frac{P_{i,j}\sum_{k = 1}^m P^{k+1}_{j,i}}{m}
        $$
        Letting $m \to \infty$ yields
        $$P_{j,i} \pi_j = P_{i,j} \pi_i$$
        which proves the theorem.
    \end{proof}
\end{theorem}

The concept of the reversed chain is useful even when the process is not time reversible. To illustrate this, we start with the following proposition.
\begin{proposition}
    Consider an irreducible Markov chain with transition probabilities
    $P_{i,j}$. If we can find positive numbers $\pi_i$ , $i \in S$, summing to one, and a transition probability matrix $Q = [Q_{i,j}]$ such that
    \begin{equation}\label{eq 4.29}
        \pi_i P_{i,j} =\pi_j Q_{j,i}
    \end{equation}
    then the $Q_{i,j}$ are the transition probabilities of the reversed chain and the $\pi_i$ are the
    stationary probabilities both for the original and reversed chain.
\end{proposition}
    

\section{Monte Carlo Markov Chains}

Given probability $\mbf{\pi}$ on $S$ and a random variable $f: S \to \mbb{R}$, we may want to compute the mean $\mbb{E}[f]=:\mbf{\pi}(f)$. Or might be given an unnormalized probability $\tilde{\pi}$ and want $\mbf{\pi}(f)$ with $\mbf{\pi} = \frac{\tilde{\mbf{\pi}}}{Z}$ and $Z$ is the normalizing coefficient $Z = \tilde{\mbf{\pi}} \vec{1}$, or might want to compute $Z$. Such problems arise in statistical mechanics where probability of state $x$ is proportional to $e^{-\beta H(x)}$ with $H$ interpreted as energy and $\beta = \frac{1}{R_BT}$ interpreted as coolness. Then the normalizing coefficient is $Z = \sum_{x\in S} e^{-\beta H(x)}$ and mean energy is 
$$
\frac{1}{Z} \sum_{x \in S} H e^{-\beta H}.
$$

They also arise in statistical inference for Bayesian model comparison. For example, suppose model $M$ has parameterd $\mu$ and consists in a specification of 
$$
\mbb{P} [\text{data} | M, \mu].
$$
Bayesian inference gives 
$$
\mbb{P} [\mu | \text{data}, M] = \frac{\mbb{P} [\text{data} | M, \mu] \mbb{P}[\mu]}{Z(M)}
$$
To compare $2$ models $M_1$ and $M_2$, we need to look at 
$$
\frac{\mbb{P}[M_1 | \text{data}]}{\mbb{P}[M_2 | \text{data}]} = \frac{Z(M_1) \mbb{P}[M_1]}{Z(M_2) \mbb{P}[M_2]}.
$$

Design a MC with unique absorbing component on which $\tilde{mbf{\pi}}$ is stationary. Then the fraction of time spent in state $x$ by a typical realization converges to $\mbf{\pi}(x)$ as $T \to \infty$, and the time-average of $\tilde{\mbf{\pi}}(X_n) \to Z$.

The easiest way to achieve $\tilde{\pi}$ stationary is to choose $P$ so that $\tilde{\pi}_i P_{i,j} = \tilde{\mbf{\pi}}_j P_{j,i}$ $\forall i,j \in S$ and $P \vec{1} = \vec{1}$ with $P_{i,j} \ge 0$. We can do this by taking any proposal transition probabilities $Q_{i,j}$ and use acceptance probabilities $A_{i,j}$ like
$$
\text{Metropolis–Hastings } A_{i,j} = 
\begin{cases}
    1 \qquad & \text{if } \tilde{\mbf{\pi}}_j Q_{j,i} \ge \tilde{\mbf{\pi}}_i Q_{i,j} \\ 
    \frac{\tilde{\mbf{\pi}}_j Q_{j,i}}{\tilde{\mbf{\pi}}_i Q_{i,j}} \qquad &\text{otherwise}
\end{cases}.
$$

$$
\text{Heatbath } A_{i,j} = \frac{\tilde{\mbf{\pi}}_j Q_{j,i}}{\tilde{\mbf{\pi}}_i Q_{i,j} + \tilde{\mbf{\pi}}_j Q_{j,i}},
$$

and set $P_{i,j} = Q_{i,j} A_{i,j}$, $j \neq i$, and $P_{i,i}$ be the reject.

We may as well take $Q_{i,i} = 0,\, \forall i \in S$, and require that $Q$ to have unique absorbing component (in general the whole of $S$). Then with the above choices, $P$ has detailed balance for $\tilde{\mbf{\pi}}$:
$$
\frac{\tilde{\mbf{\pi}}_i Q_{i,j} A_{i,j}}{\tilde{\mbf{\pi}}_j Q_{j,i}A_{j,i}} = 1.
$$

This can be done without rejection: given $i$, let $w_{i,j} = Q_{i,j} A_{i,j}$, and $W_{i} = \sum_{j \in S} w_{i,j}$. Let 
$$
P_{i,j} = \frac{w_{i,j}}{W_i},
$$ 
and weight time spent in $i$ by $W_i$. 

Question of time needed to explore the state space, or mixing time (really only need DS ergodization time, but $P^T$ is SP ergodic if $P$ is DS ergoditc and $T$ is the period, for a cyclic component).

Mixing time 
$$
T(\epsilon) = \min \{T \in \mbb{N} | d(\sigma P^n, \mbf{\pi}) \le \epsilon, \forall n \ge T \text{ and } \sigma \in \Delta\},$$
using total variation distance (which is defined as one half of $l_1$ distance).

Define $$
\|P\|_Z = \sup_{\mbf{v} \neq 0,\, \mbf{v} \vec{1} = 0} \frac{\| v P\|_1}{\|v\|_1},
$$
where $Z$ is interpreted as ``zero charge".

If $\|P\|_Z < 1$, then $P$ is a contraction on $\Delta$, so get SP-ergodicity.

If $\| P^n \|_Z \le C r^n$ with $0 < r < 1$ then $T(\epsilon) \le \frac{\log (\epsilon/c)}{\log r}$.

One can get such bounds on $\|P^n\|_Z$ from Dobrushin's ergodicity coefficients:
$$
\|P\|_Z = 1 - \min_{i,j}\sum_{k} \min\{P_{i,k}, P_{j,k}\} =  \frac{1}{2} \max_{i,j} \sum_{k} |P_{i,k} - P_{j,l}|
$$

But what is really wanted is to know $d(\mu_T, \mbf{\pi})$, where $\mu_T = \frac{1}{T} \sum_{n=0}^{T-1} \delta_{X_n}$. Typically, this is $O (T^{-1/2})$ (CLT generalization).

Can reformulate: find $\min T$ such that $\mbb{P} [d( \mu_T, \mbf{\pi}) > \epsilon] > \eta.$ Large Deviation Theorem implies 
$$\mbb{P} [d( \mu_T, \mbf{\pi}) > \epsilon]  \le C \exp (- \frac{T\epsilon^2}{K + \frac{1}{2}}),$$ 
where $K = \|(I - P)^{-1}\|_Z$. So $T \sim (K+ \frac{1}{2})\epsilon^{-2} \log \frac{C}{\eta}$.

Can adapt the MCMC to the function $f$ whose mean we want: importance sampling/variance reduction.

\section{Countable Discrete-Time Markov Chains}

One can extend much of what we've done for finite DTMC to the countably infite case, for example, the simple random walk  (SRW) on $\mbb{Z}$. But some results become more subtile, for example, $SRW$ is not SP-ergodic, despite being irreducible (actually it also fails to have a stationary probability; also it is not aperiodic --- it has a period $2$.)

One have to refine various concepts. Let
$$
T_x = \inf \{n > 0: X_n = x\}.
$$

The following definitions are the same as those defined before.
\begin{definition}[Transience]
    Say $x \in S$ is \textbf{transient} if 
$$
\mbb{P} [T_x = \infty | X_0 = x] > 0,
$$
\end{definition}

\begin{remark}
    $x$ is transient, then with probability $1$ $X_n$ comes back to $x$ only finitely many times.
\end{remark}

\begin{definition}[Null Recurrence]
    Say $x \in S$ is \textbf{null recurent} if 
$$
\mbb{P} [T_x < \infty | X_0 = x] = 1,
$$
and 
$$
\mbb{E} [T_x | X_0 = x] = \infty.
$$
\end{definition}

\begin{definition}[Positive Recurrence]
    Say $x \in S$ is \textbf{positive recurrent} if 
$$
\mbb{P} [T_x < \infty | X_0 = x] = 1,
$$
and 
$$
\mbb{E} [T_x | X_0 = x] < \infty.
$$
\end{definition}

\begin{definition}[Communicating Classes]
    A \textbf{communicating class} is either null recurrent or positive recurrent. 
\end{definition}

\begin{theorem}
    An absorbing class has a unique stationary probability if and only if it is positive recurrent. And $\mbf{\pi}_x = 1/T_x$.
\end{theorem}

\begin{remark}
    Positive recurrence guarantees the existence and uniqueness of the stationary distribution.
\end{remark}