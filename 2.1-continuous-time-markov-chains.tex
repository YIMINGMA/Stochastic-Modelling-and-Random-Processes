\section{Continuous-Time Markov Chains}

We are now considering a continuous-time markov chain with a countable state space $S$ and the domain $T \in \mbb{R}$ (or $T \in \mbb{R}_+$), and we restrict $X: \mbb{R} \mapsto S$ to those which are \textit{piecewise constant} and \textit{right-continuous}, meaning

\begin{equation*}
    X(t) = 
    \begin{cases}
        \vdots &  \vdots \\ 
        s \quad & t \in [J_s, J_{s'}) \\ 
        s' \quad & t \in [J_{s'}, J_{s''}) \\ 
        \vdots & \vdots \\ 
    \end{cases}
\end{equation*}

\begin{definition}[Continuous-Time Markov Chains]
    $X(t): \mbb{R} \mapsto S$ is a \textbf{continuous-time Markov chain}, if it satisfies the \textbf{Markov property}

    \begin{equation*}
        \Prob[X(t_{n+1}) \in A | X(t_n) = s_n, \cdots, X(t_1) =  s_1] = \Prob[X(t_{n+1}) \in A | X(t_n) = s_n],
    \end{equation*}

    where $A \subset S$ and $t_1 < \cdots t_{n} < t_{n+1}$.
\end{definition}

\begin{definition}[Homogeneity]
    A continuous-time Markov chain is \textbf{homogeneous} if 

    \begin{equation*}
        \Prob [X(t+u) \in A | X(u) = s] = \Prob [X(t) \in A | X(0) = s].
    \end{equation*}
\end{definition}

\begin{remark}
    Homogeneity means time translation invariance.
\end{remark}


\begin{definition}[Transition Matrices]
    Let $(P_t)_{i,j} := \Prob [X(t) = j | X(0) = i]$, then $P_t$ is the transition matrix with time step $t$.
\end{definition}

\begin{remark}
    The $(i,j)$ element of the transition matrix $P_t$ can also be expressed as $P_t(i, j)$.
\end{remark}

\begin{theorem}[Chapman-Kolmogorov Equation]
    The transition matrix $P$ of a homogeneous Markov chain satisfies

    \begin{equation*}
        P_{t+u} = P_t P_u, \, P_0 = I.
    \end{equation*}

    \begin{proof}
        Notice that

        \begin{align*}
            (P_{t+u})_{i,j} = & \Prob[X(t+u) = j | X(0) = i] \\ 
            = & \sum_{k \in S} \Prob[X(t+u) = j | X(t) = k, \, X(0) = i] \Prob[X(t) = k | X(0) = i] \\ 
            = & \sum_{k \in S} \Prob[X(t+u) = j | X(t) = k] \Prob[X(t) = k | X(0) = i] \\ 
            = & \sum_{k \in S} \Prob[X(u) = j | X(0) = k] \Prob[X(t) = k | X(0) = i] \\ 
            = & \sum_{k \in S} (P_u)_{k, j} (P_t)_{i, k} \\ 
            = & (P_t)_{i, :} \; (P_u)_{:, j},
        \end{align*}

        where $(P_t)_{i, :}$ is the $i$-th row of $P_t$ and $(P_u)_{:, j}$ is the $j$-th column of $P_u$. Thus, $P_{t+u} = P_t P_u$. 

        And by definition, $(P_0)_{i,j} = \Prob[X_0 = j | X_0 = i] = \delta_{i,j}$, so $P_0 = I$.
    \end{proof}
\end{theorem}

\subsection{The Rate Matrix} 

\begin{definition}[Rate Matrix]
    Suppose $P_t$ is differentiable with respect to $t$ at $t = 0$, then 

    \begin{equation*}
        G :=  \left. \frac{\dif P_t}{\dif t} \right|_{t = 0}
    \end{equation*}
    is called the \textbf{generator} or the \textbf{rate matrix} of the process.
\end{definition}

\begin{proposition}
    $P_t = \exp(tG)$ in the sense of power series.
    
    \begin{proof}
        By the Chapman-Kolmogorov equation, we have 
        
        \begin{align*}
            P_{t+u} = & P_t P_u \\ 
            P_{t+u} - P_t = & P_t (P_u- I) \\ 
            \frac{P_{t+u} - P_t}{u} = & P_t \cdot \frac{P_u - I}{u} \\ 
            \lim_{u \to 0} \frac{P_{t+u} - P_t}{u} = & \lim_{u \to 0} P_t \cdot \frac{P_u - I}{u} \\ 
            \lim_{u \to 0} \frac{P_{t+u} - P_t}{u} = &  P_t \cdot \lim_{u \to 0} \frac{P_u - I}{u} \\ 
            \frac{\dif P_t}{\dif t} = &  P_t G,
        \end{align*}

        So $P_t = C \cdot \exp(tG)$, where $C$ is a constant diagonal matrix with diagonal elements being equal. By $P_0 = I$, we konw $C = I$.
    \end{proof}
\end{proposition}

\begin{proposition}
    The generator $G$ also satisfies 
    
    \begin{equation*}
        G \vec{1} = \vec{0}.
    \end{equation*}

    \begin{proof}
        For any probability distribution $\mbf{\pi}_t = \mbf{\pi}_0 P_t$ with initial distribution $\mbf{\pi}_0$, evolves by 

        \begin{equation*}
            \frac{\dif \mbf{\pi}_t}{\dif t} = \mbf{\pi}_0 \frac{\dif P_t}{\dif t} = \mbf{\pi}_0 P_t G = \mbf{\pi}_t G.
        \end{equation*}

        And by conservation of probability, we have $\mbf{\pi}_t \vec{1} = \vec{1}$, which implies $\mbf{\pi}_t G \vec{1} = \frac{\dif \mbf{\pi}_t \vec{1}}{\dif t} = 0$. Since $\mbf{\pi}_t$ is arbitrary, we have $G \vec{1} = 0$.
    \end{proof}
\end{proposition}

\begin{theorem}[The Master Equation]
    The equation 

    \begin{equation*}
        \frac{\dif \mbf{\pi}_t}{\dif t} = \mbf{\pi}_t G
    \end{equation*}
    
    can be written into

    \begin{equation*}
        \frac{\dif (\mbf{\pi}_t)_i}{\dif t} = \underbrace{\sum_{j \neq i} (\mbf{\pi}_t)_j G_{j, i}}_{\text{``gain''}}  - \underbrace{\sum_{j \neq i} (\mbf{\pi}_t)_i G_{i, j}}_{\text{``loss''}},
    \end{equation*}

    which is called the \textbf{master equation}.

    \begin{proof}
        For $i \neq j$, since $G_{i,j}$ is the rate at which the process goes from state $i$ to $j$, we have $G_{i,j} \ge 0$. By $G \vec{1} = \vec{0}$, we have 
    
        \begin{equation*}
            G_{i, i} = - \sum_{j \neq i} G_{i, j}.
        \end{equation*}
    
        So 
    
        \begin{align*}
            \frac{\dif (\mbf{\pi}_t)_i}{\dif t} = & \mbf{\pi}_t G_{:, i} \\ 
            = & \sum_{j \in S} (\mbf{\pi}_t)_j G_{j, i} \\ 
            = & \sum_{j \neq i} (\mbf{\pi}_t)_j G_{j, i} - \sum_{j \neq i} (\mbf{\pi}_t)_i G_{i, j}.
        \end{align*}
    \end{proof}
\end{theorem}

\begin{remark}
    The name ``master equation'' is exaggerated; it does not tell everything about the process, such as the correlations between states at different times.
\end{remark}

\begin{example}[Poisson Processes]
    The \textbf{Poisson process} with rate $\lambda > 0$ has the state space $S = \mbb{N}$, $X(0) = 0$, and the transition matirx $G$ such that 

    \begin{equation*}
        G_{i,j} = \begin{cases}
            \lambda \quad & j = i+1 \\
            -\lambda \quad & j = 1
        \end{cases}.
    \end{equation*}

    It has $\mbb{P}[X(t+u) = n+k | X(u) = n] = e^{-\lambda t} \frac{(\lambda t)^k}{k!}$, $\forall n, k \in \mbb{N}$, $\forall t, u \in \mbb{R}_+$.
\end{example}

\begin{example}[Birth and Death Processes]
    Suppose we have the birth rates $\alpha_i$ and the death rates $\beta_i$ ($\beta_0 = 0$), for $i \in S = \mbb{N}$. The rate matrix $G$ is defined by 

    \begin{equation*}
        G_{i,j} = 
        \begin{cases}
            \alpha_i \quad & j = i+1 \\ 
            \beta_i \quad & j = i-1 \\ 
            - (\alpha_i + \beta_i) \quad & j = i
        \end{cases}.
    \end{equation*}

    Then the process is called the \textbf{Birth and Death Process}.  
\end{example}

\begin{example}[$M/M/1$ queue]
    The birth and death process has a special case - the \textbf{$\mbf{M/M/1}$ queue}, in which $\alpha_i = \alpha$, $\beta_i = \beta$ for $i \neq 0$ and $\beta_0 = 0$. $M$ means ``memoryless'', and $1$ means there is only one cashier to serve customers.
\end{example}

\begin{example}[$M/M/\infty$ queue]
    Another example is the \textbf{$\mbf{M/M/\infty}$ queue}, in which there are infinitely many servers so that customers do not have to wait for people in front of them. In this model $\alpha_i = \alpha$ and $\beta = i \beta$.
\end{example}

\begin{example}[Population Growth]
    Population growth can be modelled by the birth and death process with $\alpha_i = i \alpha$ and $\beta_i = i\beta$, where $i$ is the size of population.
\end{example}

\subsection{Stationarity and Reversibility}

\begin{definition}[Stationarity]
    Say $\mbf{\pi} \in \Delta$ is \textbf{stationary} if $\mbf{\pi} G = 0$.
\end{definition}

\begin{definition}[Reversibility]
    Say $\mbf{\pi} \in \Delta$ is \textbf{reversible} if 

    \begin{equation*}
        \mbf{\pi}_i G_{i,j} = \mbf{\pi}_j G_{j, i}, \; \forall i,j \in S.
    \end{equation*}
\end{definition}

\begin{proposition}[$\text{Reversibility} \implies \text{Stationarity}$]
    If $\mbf{\pi} \in \Delta$ is reversibile, then it is also stationary.
\end{proposition}

\begin{proposition}
    $S$ is fintie $\implies$ $\exists$ stationary $\mbf{\pi}$.
\end{proposition}

There is an analogous decomposition of the state space $S$ into transient and recurrent states, and of the set of recurrent states into communicating components. And we have the same definition of an absorbing component. 

\begin{proposition}
    If $S$ is finite, then each absorbing component has a unique stationary probability $\mbf{\pi}$, and the space of starionary $\mbf{\pi}$ for the whole continuous-time Markov chain (up to normalisation) is the span of those for its absorbing components. Furthermore, $0$ is a semisimple eigenvalue of $G$.
\end{proposition}

\begin{theorem}
    Suppose $S$ is finite and $G$ has a unique absorbing component, then the process is SP-ergodic, which means 

    \begin{equation*}
        \lim_{t \to \infty} \mbf{\pi}_t = \mbf{\pi}_A, 
    \end{equation*}

    where $\mbf{\pi}_A$ is the stationary distribution of the absorbing component.
\end{theorem}

\begin{remark}
    Aperiodicity is automatic in continuous time.
\end{remark}

\subsection{The Jump Chain}

\begin{definition}[Waiting Times]
    The \textbf{waiting time} or the \textbf{holding time} $W_x$ is defined as 

    \begin{equation*}
        W_x = \inf\{t > 0: X(t) \neq x | X(0) = x \}.
    \end{equation*}
\end{definition}

\begin{proposition}
    The waiting time $W_x$ is exponentially distributed with mean $\frac{1}{|G_{x,x}|}$.

    \begin{proof}
        \begin{align*}
            \Prob [W_x > t + u | W_x  > t] = & \Prob [W_x > t+u | X(s) = x, \, \forall s \le t] \\ 
            = & \Prob [W_x > t + u | X(t) = x] \\ 
            = & \Prob [W_x > u | X(0) = x] \\ 
            = & \Prob [W_x > u].
        \end{align*}

        So $\Prob [W_x > t + u] = \Prob [W_x > u] \Prob [W_x > t]$. So $\exists \gamma \in \mbb{R}$, such that
        
        \begin{equation*}
            \Prob[W_x > t] = e^{- \gamma t}.
        \end{equation*}

        $\left. \frac{\dif}{\dif t} \Prob[W_x > t] \right|_{t = 0} = G_{x,x}$ shows $\gamma = - G_{x,x}$. 
    \end{proof}
\end{proposition}

\begin{definition}[Jump Times]
    Define \textbf{jump times} $J_{n+1} = \inf \{ t > J_n: X(t) \neq X(J_n)\}$, with $J_0 = 0$.
\end{definition}

\begin{remark}
    The jump times are an example of ``stopping times'', i.e. random variables such that $\{J_n \le t\}$ is independent of $\{ X(s): s > t\}$ given $\{ X(s): s \le t\}$.
\end{remark}

\begin{theorem}
    Markov chains satisfiy the \textbf{strong Markov property}: let $T$ be a stopping time conditional on $X_T = i$, then $X_{T+t}$ ($t \ge 0$) is Markov and independent of $\{X(s): s \le T\}$.
\end{theorem}

\begin{definition}[The Jump Chain]
    Let $Y_n = X(J_n)$, then $\{Y_n: n \in \mbb{N}\}$ is called the \textbf{jump chain} of $\{X_t: t \in \mbb{R}\}$.
\end{definition}

\begin{remark}
    The jump chain $\{Y_n: n \in \mbb{N}\}$ is a discrete-time Markov chain.
\end{remark}

\begin{proposition}
    The one-step transition matrix of the jump chain $\{Y_n: n \in \mbb{N}\}$ is

    \begin{equation*}
        P_{i,j} = 
        \begin{cases}
            0 \quad & j = i \\ 
            \frac{G_{i,j}}{|G_{i,i}|} \quad & j \neq i \, \& \, G_{i,i} = 0 \\ 
            \delta_{i.,} \quad & G_{i,i} = 0
        \end{cases}.
    \end{equation*}
\end{proposition}

\begin{remark}
    We can make sample paths for the continuous-time Markov chain by making paths for the associated jump chain and choosing independent waiting times $W_{Y_n}$ with mean $1 / |G_{Y_n, Y_n}|$, and let

    \begin{equation*}
        J_n = \sum_{0 \le k < n} W_{Y_k}.
    \end{equation*}
\end{remark}
